---
title: "1장. 텍스트마이닝"
output:
  md_document:
    variant: markdown_github
---

### 1절. 데이터 전처리

#### 1. tm 패키지

문서 관리의 기본 구조인 Corpus를 생성하여 tm_map 함수를 통해 데이터 전처리 및 가공

Corpus와 VCorpus 중 에러가 적게 나는 VCorpus 사용

-   Corpus : 데이터마이닝 절차 중 데이터의 정제, 통합, 선택, 변환의 과정을 거친 구조화된 단계로 데이터마이닝 알고리즘 실험에 활용될 수 있는 상태

    ※ VCorpus : 문서를 Corpus class로 만들어주는 함수, 결과는 현재 구동중인 R메모리에서만 유지

-   tm 패키지

    tm패키지(Corpus 생성) -\> Corpus전처리 -\> 분석에 활용

    VectorSource함수(텍스트 데이터 -\> 문서) -\> VCorpus함수(문서 데이터 -\> Corpus)

-   R

    -   VectorSource(text) : 텍스트 데이터 -\> 문서로 변경

        -   text : 문서로 변경하고자 하는 텍스트 데이터

    -   VCorpus(data) : 문서 데이터 -\> Corpus 변경

        -   data : VectorSource 함수를 실행한 데이터

```{r}
# 텍스트 데이터

#install.packages('tm')
library(tm)

data(crude)
summary(crude)[1:6,] # 데이터의 형태 파악
inspect(crude[1]) # 문서의 정보(파일 형태, 글자 수 등)을 확인
crude[[1]]$content # 문서의 내용 확인
```

-   tm_map(x, FUN) : 텍스트 전처리

    -   FUN 함수\
        tolower : 소문자로 변환\
        stemDocument : 어근만 남기기\
        stripDocument : 공백제거\
        removePunctuation : 문장부호제거\
        removeNumbers : 숫자제거\
        removeWords,'word' : 단어제거\
        removeWord,stopwords('word') : 불용어제거\
        PlainTextDocument : TextDocument로 변환

```{r}
library(tm)

news <- readLines('./news_title.txt')
news

news.corpus <- VCorpus(VectorSource(news)) # 데이터 -> 문서 -> Corpus
news.corpus[[1]]$content

# 텍스트 전처리 함수
clean_txt <- function(txt){
  txt <- tm_map(txt, removeNumbers) # 숫자 제거
  txt <- tm_map(txt, removePunctuation) # 문장부호 제거(',•,... 은 제외)
  txt <- tm_map(txt, stripWhitespace) # 공백제거 
  return(txt)
}

clean.news <- clean_txt(news.corpus)
clean.news[[1]]$content

txt2 <- gsub('[[:punct:]]','',clean.news[[1]])
txt2
```

※ gsup : 해당 단어 찾아바꾸기\
[[:punct:]] : 특수문자\
[[:digit:]] : 숫자\
[[A-z]] : 알파벳\
[[:alnum:]] : 영문자/숫자

#### 2. 자연어 처리 ~~**\# KoNLP 패키지**~~

한글 자연어 처리 분석 -\> KoNLP 패키지 =\> 자연어 처리 및 텍스트 마이닝

※ KoNLP패키지를 활용한 한글 처리\
자연어 처리에 앞서 품사 등을 포함하고 있는 사전을 설정 (주로 SejongDic을 등록)\
buildDictionart()함수 : 직접 사전에 추가\
SimplePos22()함수: (문장-\>형태소) 분리하여 22개의 품사 태그를 달아줌\
extraNoun()함수 : 문장에서 명사만 추출

-   buildDictionary(ext_dic, data) : 단어를 직접 사전에 추가

    -   ext_dic : 단어를 추가하고자 하는 사전을 선택('woorimalsam','sejong','insighter')

    -   data : 추가하고자 하는단어와 품사가 들어간 data.frame or txt파일

-   SimplePos22(text) : 형태소로 분리

-   extraNoun(text) : 문장에서 명사 추출

```{r}
#install.packages("KoNLP")
```

=\> 현재 KoNLP 패키지가 다운이 안됨...

-   stemming : 어간 추출(공통 어간을 가지는 단어를 묶는 작업) \# **tm패키지**

    -   stemDocument(text) : 공통으로 들어가지 않은 부분 제외

    -   stemCompletion(text, dictionary) : 가장 기본적인 어휘로 완성

```{r}
#install.packages('SnowballC')
library(SnowballC)

library(tm)
test <- stemDocument(c('analyzed','analyzed','analyzing'))
completion <- stemCompletion(test, dictionary = c('analyze','analyzed','analyzing'))
completion
```

### 2절. Term-Document Matrix

TDM(Term-Document Matrix) : 전처리된 데이터에서 각 문서와 단어 간의 사용 여부를 이용해 만들어진 matrix\
=\> 문서마다 등장한 단어의 빈도수를 쉽게 파악 가능

#### 1. R을 활용한 TDM 구축하기 **\# tm패키지**

-   TermDocumentMatrix(data, control)

    -   data : Corpus 형태의 데이터

    -   control : 사전 변경, 가중치 부여 등의 옵션 추가기능 지원

```{r}
library(tm)

VC.news <- VCorpus(VectorSource(clean.news))
VC.news[[2]]$content

TDM.news <- TermDocumentMatrix(VC.news)
dim(TDM.news) # 6개의 뉴스 제목에서 25개의 단어 추출

inspect(TDM.news[1:5,]) # 각 기사에서의 단어 빈도
```

※ sparsity : 전체 행렬에서 0이 차지하는 비중\
(25/30 = 83%)

#### 2. TDM을 활용한 분석 및 시각화

-   연관성 분석

    특정 단어와의 연관성에 따라 단어를 조회

    -   findAssocs(data, terms, corlimit)

        -   data : TDM 형태의 데이터

        -   terms : 연관성을 확인할 단어

        -   corlimit : 최소 연관성

```{r}
library(tm)
TDM.news2 <- TermDocumentMatrix(VC.news)

findAssocs(TDM.news2,'물가',0.9) # 텍스트가 적어서 안나옴
```

-   워드 클라우드 \# **wordcloud 패키지**

    문서에 포함되는 단어의 사용 빈도를 효과적으로 보여주기 위한 시각화

    -   wordcloud(words, freq, min.freq, random.order, colors, ...)

        -   words : 워드클라우드를 만들고자하는 단어

        -   freq : 단어의 빈도

        -   min.freq : 시각화하려는 단어의 최소빈도

        -   random.order : 단어의 배치를 랜덤으로 할지 정함. (F=빈도순)

        -   colors : 빈도에 따라 단어의 색을 지정

```{r warning=FALSE}
#install.packages('wordcloud')
library(wordcloud)

tdm2 <- as.matrix(TDM.news2)
term.freq <- sort(rowSums(tdm2),decreasing = T)
head(term.freq,5)

wordcloud(words = names(term.freq))
```
