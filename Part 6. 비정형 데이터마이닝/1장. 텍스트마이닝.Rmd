---
title: "1장. 텍스트마이닝"
output:
  md_document:
    variant: markdown_github
---

```{r}
rm(list = ls())
```

### 1절. 데이터 전처리

#### 1. tm 패키지

문서 관리의 기본 구조인 Corpus를 생성하여 tm_map 함수를 통해 데이터 전처리 및 가공

Corpus와 VCorpus 중 에러가 적게 나는 VCorpus 사용

-   Corpus : 데이터마이닝 절차 중 데이터의 정제, 통합, 선택, 변환의 과정을 거친 구조화된 단계로 데이터마이닝 알고리즘 실험에 활용될 수 있는 상태

    ※ VCorpus : 문서를 Corpus class로 만들어주는 함수, 결과는 현재 구동중인 R메모리에서만 유지

-   tm 패키지

    tm패키지(Corpus 생성) -\> Corpus전처리 -\> 분석에 활용

    VectorSource함수(텍스트 데이터 -\> 문서) -\> VCorpus함수(문서 데이터 -\> Corpus)

-   R

    -   VectorSource(text) : 텍스트 데이터 -\> 문서로 변경

        -   text : 문서로 변경하고자 하는 텍스트 데이터

    -   VCorpus(data) : 문서 데이터 -\> Corpus 변경

        -   data : VectorSource 함수를 실행한 데이터

```{r}
# 텍스트 데이터

#install.packages('tm')
library(tm)

data(crude)
summary(crude)[1:6,] # 데이터의 형태 파악
inspect(crude[1]) # 문서의 정보(파일 형태, 글자 수 등)을 확인
crude[[1]]$content # 문서의 내용 확인
```

-   tm_map(x, FUN) : 텍스트 전처리

    -   FUN 함수\
        tolower : 소문자로 변환\
        stemDocument : 어근만 남기기\
        stripDocument : 공백제거\
        removePunctuation : 문장부호제거\
        removeNumbers : 숫자제거\
        removeWords,'word' : 단어제거\
        removeWord,stopwords('word') : 불용어제거\
        PlainTextDocument : TextDocument로 변환

```{r}
library(tm)

news <- readLines('./news_title.txt')
news

news.corpus <- VCorpus(VectorSource(news)) # 데이터 -> 문서 -> Corpus
news.corpus[[1]]$content

# 텍스트 전처리 함수
clean_txt <- function(txt){
  txt <- tm_map(txt, removeNumbers) # 숫자 제거
  txt <- tm_map(txt, removePunctuation) # 문장부호 제거(',•,... 은 제외)
  txt <- tm_map(txt, stripWhitespace) # 공백제거 
  return(txt)
}

clean.news <- clean_txt(news.corpus)
clean.news[[1]]$content

txt2 <- gsub('[[:punct:]]','',clean.news[[1]])
txt2
```

※ gsup : 해당 단어 찾아바꾸기\
[[:punct:]] : 특수문자\
[[:digit:]] : 숫자\
[[A-z]] : 알파벳\
[[:alnum:]] : 영문자/숫자

#### 2. 자연어 처리 **\# KoNLP 패키지**

한글 자연어 처리 분석 -\> KoNLP 패키지 =\> 자연어 처리 및 텍스트 마이닝

※ KoNLP패키지를 활용한 한글 처리\
자연어 처리에 앞서 품사 등을 포함하고 있는 사전을 설정 (주로 SejongDic을 등록)\
buildDictionart()함수 : 직접 사전에 추가\
SimplePos22()함수: (문장-\>형태소) 분리하여 22개의 품사 태그를 달아줌\
extraNoun()함수 : 문장에서 명사만 추출

-   buildDictionary(ext_dic, data) : 단어를 직접 사전에 추가

    -   ext_dic : 단어를 추가하고자 하는 사전을 선택('woorimalsam','sejong','insighter')

    -   data : 추가하고자 하는단어와 품사가 들어간 data.frame or txt파일

-   SimplePos22(text) : 형태소로 분리

-   extraNoun(text) : 문장에서 명사 추출

```{r}
library(KoNLP)

useSejongDic()
```

\
=\> ~~현재 KoNLP 패키지가 다운이 안됨...\
~~(23.04.06) KoNLP 패키지 설치 방법 - KoNLP패키지.R을 통해

```{r}
sentence <- '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence) # '스르륵'은 명사X (부사)

buildDictionary(ext_dic = 'sejong',
                user_dic = data.frame(c('스르륵'),c('mag')))
extractNoun(sentence)

SimplePos22(sentence)

```

※ 품사 태크

|  **품사**  |                                                       중간 품사                                                        |
|:----------:|:----------------------------------------------------------------------------------------------------------------------:|
|  S : 기호  | sp:쉼표, sf:마침표, sl:여는 따옴표 및 묶음표, sr:닫는 따옴표 및 묶음표, sd:이음표, se:줄임표, su:단위기호, sy:기타기호 |
| F : 외국어 |                                                        f:외국어                                                        |
|  N : 체언  |                               NC:보통명사, NQ:고유명사, NB:의존명사, NP:대명사, NN:수사                                |
|  P : 용언  |                                            PV:동사, PA:형용사, PX:보조용언                                             |
| M : 수식언 |                                                   MM:관형사, MA:부사                                                   |
| I : 독립언 |                                                       II:감탄사                                                        |
| J : 관계언 |                                          JC:격조사, JX:보조사, JP:서술격조사                                           |
|  E : 어미  |                                  EP:선어말어미, EC:연결어미, ET:전선어미, EF:종결어미                                  |
|  X : 접사  |                                                  XP:접두사, XS:접미사                                                  |

```{r}
clean_txt2 <- function(txt){
  txt <- removeNumbers(txt) # 숫자 제거
  txt <- removePunctuation(txt) # 문장부호 제거
  txt <- stripWhitespace(txt) # 공백 제거
  txt <- gsub("[^[:alnum:]]"," ",txt) # 영문자/숫자를 제외한 것들 " "로 처리
  return(txt)
}

clean.news2 <- clean_txt2(news)
clean.news2
```

```{r}
doc1 <- paste(SimplePos22(clean.news2[[1]])) # 리스트 형태 -> 벡터로
doc1

# 형용사 추출
library(stringr)
doc2 <- str_match(doc1,'([가-힣]+)/PA') 
doc2

doc3 <- doc2[,2]
doc3[!is.na(doc3)]
```

-   stemming : 어간 추출(공통 어간을 가지는 단어를 묶는 작업) \# **tm패키지**

    -   stemDocument(text) : 공통으로 들어가지 않은 부분 제외

    -   stemCompletion(text, dictionary) : 가장 기본적인 어휘로 완성

```{r}
#install.packages('SnowballC')
library(SnowballC)

library(tm)
test <- stemDocument(c('analyzed','analyzed','analyzing'))
completion <- stemCompletion(test, dictionary = c('analyze','analyzed','analyzing'))
completion
```

### 2절. Term-Document Matrix

TDM(Term-Document Matrix) : 전처리된 데이터에서 각 문서와 단어 간의 사용 여부를 이용해 만들어진 matrix\
=\> 문서마다 등장한 단어의 빈도수를 쉽게 파악 가능

#### 1. R을 활용한 TDM 구축하기 **\# tm패키지**

-   TermDocumentMatrix(data, control)

    -   data : Corpus 형태의 데이터

    -   control : 사전 변경, 가중치 부여 등의 옵션 추가기능 지원

```{r}
library(tm)

VC.news <- VCorpus(VectorSource(clean.news))
VC.news[[2]]$content

TDM.news <- TermDocumentMatrix(VC.news)
dim(TDM.news) # 6개의 뉴스 제목에서 25개의 단어 추출

inspect(TDM.news[1:5,]) # 각 기사에서의 단어 빈도
```

※ sparsity : 전체 행렬에서 0이 차지하는 비중\
(25/30 = 83%)

```{r}
# 명사만 추출하여 TDM 만드는 사용자 정의 함수
words <- function(doc){
  doc <- as.character(doc)
  extractNoun(doc)
}

TDM.news2 <- TermDocumentMatrix(VC.news, control = list(tokenize=words))
dim(TDM.news2) # 6개의 기사 제목에서 12개의 단어 추출
inspect(TDM.news2)

tdm2 <- as.matrix(TDM.news2)
tdm3 <- rowSums(tdm2)
tdm4 <- tdm3[order(tdm3,decreasing = T)]
tdm4[1:10]
```

```{r}
# 단어 사전 정의하여 해당 단어들에 대해서만 분석
mydict <- c('경제', '농산물')

my.news <- TermDocumentMatrix(VC.news,control = list(tokenize=words, dictionary=mydict))
inspect(my.news)
```

#### 2. TDM을 활용한 분석 및 시각화

-   연관성 분석

    특정 단어와의 연관성에 따라 단어를 조회

    -   findAssocs(data, terms, corlimit)

        -   data : TDM 형태의 데이터

        -   terms : 연관성을 확인할 단어

        -   corlimit : 최소 연관성

```{r}
# 명사만 추출하여 TDM 만드는 사용자 정의 함수
words <- function(doc){
  doc <- as.character(doc)
  extractNoun(doc)
}


TDM.news2 <- TermDocumentMatrix(VC.news, control = list(tokenize=words))

findAssocs(TDM.news2,'물가',0.9) # 텍스트가 적어서 안나옴
```

-   워드 클라우드 \# **wordcloud 패키지**

    문서에 포함되는 단어의 사용 빈도를 효과적으로 보여주기 위한 시각화

    -   wordcloud(words, freq, min.freq, random.order, colors, ...)

        -   words : 워드클라우드를 만들고자하는 단어

        -   freq : 단어의 빈도

        -   min.freq : 시각화하려는 단어의 최소빈도

        -   random.order : 단어의 배치를 랜덤으로 할지 정함. (F=빈도순)

        -   colors : 빈도에 따라 단어의 색을 지정

```{r warning=FALSE}
#install.packages('wordcloud')
library(wordcloud)

tdm2 <- as.matrix(TDM.news2)
term.freq <- sort(rowSums(tdm2),decreasing = T)
head(term.freq,5)

wordcloud(words = names(term.freq), 
          family="AppleGothic") # 맥북 한글 깨질시
```
