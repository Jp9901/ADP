### 1절. 데이터 전처리

#### 1. tm 패키지

문서 관리의 기본 구조인 Corpus를 생성하여 tm_map 함수를 통해 데이터
전처리 및 가공

Corpus와 VCorpus 중 에러가 적게 나는 VCorpus 사용

-   Corpus : 데이터마이닝 절차 중 데이터의 정제, 통합, 선택, 변환의
    과정을 거친 구조화된 단계로 데이터마이닝 알고리즘 실험에 활용될 수
    있는 상태

    ※ VCorpus : 문서를 Corpus class로 만들어주는 함수, 결과는 현재
    구동중인 R메모리에서만 유지

-   tm 패키지

    tm패키지(Corpus 생성) -\> Corpus전처리 -\> 분석에 활용

    VectorSource함수(텍스트 데이터 -\> 문서) -\> VCorpus함수(문서 데이터
    -\> Corpus)

-   R

    -   VectorSource(text) : 텍스트 데이터 -\> 문서로 변경

        -   text : 문서로 변경하고자 하는 텍스트 데이터

    -   VCorpus(data) : 문서 데이터 -\> Corpus 변경

        -   data : VectorSource 함수를 실행한 데이터

``` r
# 텍스트 데이터

#install.packages('tm')
library(tm)
```

    ## Loading required package: NLP

``` r
data(crude)
summary(crude)[1:6,] # 데이터의 형태 파악
```

    ##     Length Class             Mode
    ## 127 2      PlainTextDocument list
    ## 144 2      PlainTextDocument list
    ## 191 2      PlainTextDocument list
    ## 194 2      PlainTextDocument list
    ## 211 2      PlainTextDocument list
    ## 236 2      PlainTextDocument list

``` r
inspect(crude[1]) # 문서의 정보(파일 형태, 글자 수 등)을 확인
```

    ## <<VCorpus>>
    ## Metadata:  corpus specific: 0, document level (indexed): 0
    ## Content:  documents: 1
    ## 
    ## $`reut-00001.xml`
    ## <<PlainTextDocument>>
    ## Metadata:  15
    ## Content:  chars: 527

``` r
crude[[1]]$content # 문서의 내용 확인
```

    ## [1] "Diamond Shamrock Corp said that\neffective today it had cut its contract prices for crude oil by\n1.50 dlrs a barrel.\n    The reduction brings its posted price for West Texas\nIntermediate to 16.00 dlrs a barrel, the copany said.\n    \"The price reduction today was made in the light of falling\noil product prices and a weak crude oil market,\" a company\nspokeswoman said.\n    Diamond is the latest in a line of U.S. oil companies that\nhave cut its contract, or posted, prices over the last two days\nciting weak oil markets.\n Reuter"

-   tm_map(x, FUN) : 텍스트 전처리

    -   FUN 함수  
        tolower : 소문자로 변환  
        stemDocument : 어근만 남기기  
        stripDocument : 공백제거  
        removePunctuation : 문장부호제거  
        removeNumbers : 숫자제거  
        removeWords,‘word’ : 단어제거  
        removeWord,stopwords(‘word’) : 불용어제거  
        PlainTextDocument : TextDocument로 변환

``` r
library(tm)

news <- readLines('./news_title.txt')
news
```

    ## [1] "\"이런 사과값은 없었다\"…물가 지표 뒤흔든 과일값"                
    ## [2] "209조 공공조달 시장 재정비…'공공조달 법률' 제정 추진"            
    ## [3] "“인구 절벽·기후 위기 대비해야”…금융위, 미래대응금융 TF 발족"     
    ## [4] "효성 '포스트 조석래' 시대…형제 독립경영 향배는?"                 
    ## [5] "한화, 호주 방산업체 오스탈 인수 추진… “사업·재무·설계 등 시너지”"
    ## [6] "농식품부 \"이달부터 농산물 물가 빠르게 개선 전망\"(종합)"

``` r
news.corpus <- VCorpus(VectorSource(news)) # 데이터 -> 문서 -> Corpus
news.corpus[[1]]$content
```

    ## [1] "\"이런 사과값은 없었다\"…물가 지표 뒤흔든 과일값"

``` r
# 텍스트 전처리 함수
clean_txt <- function(txt){
  txt <- tm_map(txt, removeNumbers) # 숫자 제거
  txt <- tm_map(txt, removePunctuation) # 문장부호 제거(',•,... 은 제외)
  txt <- tm_map(txt, stripWhitespace) # 공백제거 
  return(txt)
}

clean.news <- clean_txt(news.corpus)
clean.news[[1]]$content
```

    ## [1] "이런 사과값은 없었다…물가 지표 뒤흔든 과일값"

``` r
txt2 <- gsub('[[:punct:]]','',clean.news[[1]])
txt2
```

    ## [1] "이런 사과값은 없었다물가 지표 뒤흔든 과일값"

※ gsup : 해당 단어 찾아바꾸기  
\[\[:punct:\]\] : 특수문자  
\[\[:digit:\]\] : 숫자  
\[\[A-z\]\] : 알파벳  
\[\[:alnum:\]\] : 영문자/숫자

#### 2. 자연어 처리 **\# KoNLP 패키지**

한글 자연어 처리 분석 -\> KoNLP 패키지 =\> 자연어 처리 및 텍스트 마이닝

※ KoNLP패키지를 활용한 한글 처리  
자연어 처리에 앞서 품사 등을 포함하고 있는 사전을 설정 (주로 SejongDic을
등록)  
buildDictionart()함수 : 직접 사전에 추가  
SimplePos22()함수: (문장-\>형태소) 분리하여 22개의 품사 태그를 달아줌  
extraNoun()함수 : 문장에서 명사만 추출

-   buildDictionary(ext_dic, data) : 단어를 직접 사전에 추가

    -   ext_dic : 단어를 추가하고자 하는 사전을
        선택(‘woorimalsam’,‘sejong’,‘insighter’)

    -   data : 추가하고자 하는단어와 품사가 들어간 data.frame or txt파일

-   SimplePos22(text) : 형태소로 분리

-   extraNoun(text) : 문장에서 명사 추출

``` r
library(KoNLP)
```

    ## Checking user defined dictionary!

``` r
useSejongDic()
```

    ## Backup was just finished!
    ## 370957 words dictionary was built.

  
=\> ~~현재 KoNLP 패키지가 다운이 안됨…  
~~(23.04.06) KoNLP 패키지 설치 방법 - KoNLP패키지.R을 통해

``` r
sentence <- '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence) # '스르륵'은 명사X (부사)
```

    ## [1] "아버지" "방"     "스르륵"

``` r
buildDictionary(ext_dic = 'sejong',
                user_dic = data.frame(c('스르륵'),c('mag')))
```

    ## 370959 words dictionary was built.

``` r
extractNoun(sentence)
```

    ## [1] "아버지" "방"

``` r
SimplePos22(sentence)
```

    ## $아버지가
    ## [1] "아버지/NC+가/JC"
    ## 
    ## $방에
    ## [1] "방/NC+에/JC"
    ## 
    ## $스르륵
    ## [1] "스르륵/MA"
    ## 
    ## $들어가신다
    ## [1] "들/PV+어/EC+가/PX+시/EP+ㄴ다/EF"
    ## 
    ## $.
    ## [1] "./SF"

※ 품사 태크

|  **품사**  |                                                       중간 품사                                                        |
|:-------------------:|:-------------------------------------------------:|
|  S : 기호  | sp:쉼표, sf:마침표, sl:여는 따옴표 및 묶음표, sr:닫는 따옴표 및 묶음표, sd:이음표, se:줄임표, su:단위기호, sy:기타기호 |
| F : 외국어 |                                                        f:외국어                                                        |
|  N : 체언  |                               NC:보통명사, NQ:고유명사, NB:의존명사, NP:대명사, NN:수사                                |
|  P : 용언  |                                            PV:동사, PA:형용사, PX:보조용언                                             |
| M : 수식언 |                                                   MM:관형사, MA:부사                                                   |
| I : 독립언 |                                                       II:감탄사                                                        |
| J : 관계언 |                                          JC:격조사, JX:보조사, JP:서술격조사                                           |
|  E : 어미  |                                  EP:선어말어미, EC:연결어미, ET:전선어미, EF:종결어미                                  |
|  X : 접사  |                                                  XP:접두사, XS:접미사                                                  |

``` r
clean_txt2 <- function(txt){
  txt <- removeNumbers(txt) # 숫자 제거
  txt <- removePunctuation(txt) # 문장부호 제거
  txt <- stripWhitespace(txt) # 공백 제거
  txt <- gsub("[^[:alnum:]]"," ",txt) # 영문자/숫자를 제외한 것들 " "로 처리
  return(txt)
}

clean.news2 <- clean_txt2(news)
clean.news2
```

    ## [1] "이런 사과값은 없었다 물가 지표 뒤흔든 과일값"                   
    ## [2] "조 공공조달 시장 재정비 공공조달 법률 제정 추진"                
    ## [3] " 인구 절벽 기후 위기 대비해야  금융위 미래대응금융 TF 발족"     
    ## [4] "효성 포스트 조석래 시대 형제 독립경영 향배는"                   
    ## [5] "한화 호주 방산업체 오스탈 인수 추진   사업 재무 설계 등 시너지 "
    ## [6] "농식품부 이달부터 농산물 물가 빠르게 개선 전망종합"

``` r
doc1 <- paste(SimplePos22(clean.news2[[1]])) # 리스트 형태 -> 벡터로
doc1
```

    ## [1] "이런/MM"           "사과값/NC+은/JX"   "없/PA+었/EP+다/EF"
    ## [4] "물/NC+가/JC"       "지표/NC"           "뒤흔들/PV+ㄴ/ET"  
    ## [7] "과일/NC"           "값/NC"

``` r
# 형용사 추출
library(stringr)
doc2 <- str_match(doc1,'([가-힣]+)/PA') 
doc2
```

    ##      [,1]    [,2]
    ## [1,] NA      NA  
    ## [2,] NA      NA  
    ## [3,] "없/PA" "없"
    ## [4,] NA      NA  
    ## [5,] NA      NA  
    ## [6,] NA      NA  
    ## [7,] NA      NA  
    ## [8,] NA      NA

``` r
doc3 <- doc2[,2]
doc3[!is.na(doc3)]
```

    ## [1] "없"

-   stemming : 어간 추출(공통 어간을 가지는 단어를 묶는 작업) \#
    **tm패키지**

    -   stemDocument(text) : 공통으로 들어가지 않은 부분 제외

    -   stemCompletion(text, dictionary) : 가장 기본적인 어휘로 완성

``` r
#install.packages('SnowballC')
library(SnowballC)

library(tm)
test <- stemDocument(c('analyzed','analyzed','analyzing'))
completion <- stemCompletion(test, dictionary = c('analyze','analyzed','analyzing'))
completion
```

    ##    analyz    analyz    analyz 
    ## "analyze" "analyze" "analyze"

### 2절. Term-Document Matrix

TDM(Term-Document Matrix) : 전처리된 데이터에서 각 문서와 단어 간의 사용
여부를 이용해 만들어진 matrix  
=\> 문서마다 등장한 단어의 빈도수를 쉽게 파악 가능

#### 1. R을 활용한 TDM 구축하기 **\# tm패키지**

-   TermDocumentMatrix(data, control)

    -   data : Corpus 형태의 데이터

    -   control : 사전 변경, 가중치 부여 등의 옵션 추가기능 지원

``` r
library(tm)

VC.news <- VCorpus(VectorSource(clean.news))
VC.news[[2]]$content
```

    ## [1] "조 공공조달 시장 재정비…공공조달 법률 제정 추진"

``` r
TDM.news <- TermDocumentMatrix(VC.news)
dim(TDM.news) # 6개의 뉴스 제목에서 25개의 단어 추출
```

    ## [1] 25  6

``` r
inspect(TDM.news[1:5,]) # 각 기사에서의 단어 빈도
```

    ## <<TermDocumentMatrix (terms: 5, documents: 6)>>
    ## Non-/sparse entries: 5/25
    ## Sparsity           : 83%
    ## Maximal term length: 9
    ## Weighting          : term frequency (tf)
    ## Sample             :
    ##                  Docs
    ## Terms             1 2 3 4 5 6
    ##   “사업·재무·설계 0 0 0 0 1 0
    ##   “인구           0 0 1 0 0 0
    ##   공공조달        0 1 0 0 0 0
    ##   과일값          1 0 0 0 0 0
    ##   농산물          0 0 0 0 0 1

※ sparsity : 전체 행렬에서 0이 차지하는 비중  
(25/30 = 83%)

#### 2. TDM을 활용한 분석 및 시각화

-   연관성 분석

    특정 단어와의 연관성에 따라 단어를 조회

    -   findAssocs(data, terms, corlimit)

        -   data : TDM 형태의 데이터

        -   terms : 연관성을 확인할 단어

        -   corlimit : 최소 연관성

``` r
library(tm)
TDM.news2 <- TermDocumentMatrix(VC.news)

findAssocs(TDM.news2,'물가',0.9) # 텍스트가 적어서 안나옴
```

    ## $물가
    ## numeric(0)

-   워드 클라우드 \# **wordcloud 패키지**

    문서에 포함되는 단어의 사용 빈도를 효과적으로 보여주기 위한 시각화

    -   wordcloud(words, freq, min.freq, random.order, colors, …)

        -   words : 워드클라우드를 만들고자하는 단어

        -   freq : 단어의 빈도

        -   min.freq : 시각화하려는 단어의 최소빈도

        -   random.order : 단어의 배치를 랜덤으로 할지 정함. (F=빈도순)

        -   colors : 빈도에 따라 단어의 색을 지정

``` r
#install.packages('wordcloud')
library(wordcloud)
```

    ## Loading required package: RColorBrewer

``` r
tdm2 <- as.matrix(TDM.news2)
term.freq <- sort(rowSums(tdm2),decreasing = T)
head(term.freq,5)
```

    ## “사업·재무·설계           “인구        공공조달          과일값          농산물 
    ##               1               1               1               1               1

``` r
wordcloud(words = names(term.freq))
```

![](1장.-텍스트마이닝_files/figure-markdown_github/unnamed-chunk-10-1.png)
