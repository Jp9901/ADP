``` r
# credit card
credit <- read.csv('german_credit.csv')
credit$Creditability <- as.factor(credit$Creditability)

set.seed(1)
idx <- sample(1:nrow(credit),nrow(credit)*0.7,replace=F)
train <- credit[idx,]
test <- credit[-idx,]

# iris
data(iris)

set.seed(1)
idx <- sample(1:nrow(iris),nrow(iris)*0.7,replace=F)
train.iris <- iris[idx,]
test.iris <- iris[-idx,]
```

### 4절. SVM(Support Vector Machine) (e1071 패키지)

#### 1. 작동 원리

여러 경계 중 가장 큰 폭을 가진 경계를 찾아 분류/회귀

#### 2. R을 이용한 SVM 분석

-   svm(formula, data, kernel, gamma, cost,…)

    -   kernel : 훈련과 예측에 사용되는 커널
        (‘radial’,‘linear’,‘polynomial’,‘sigmoid’)

    -   gamma : 초평면의 기울기 (default = 1/p)

    -   cost : 과적합을 막는 정도 (default = 1)

-   tune.svm(formula, data, kernel, gamma, cost,…)\\  
    SVM 모형에서 최적의 파라미터 값을 찾아내기 위해 사용되는 함수

``` r
library(e1071)
tune.svm(Creditability~.,
         data = credit,
         gamma = 10^(-6:-1),
         cost = 10^(1:2))
```

    ## 
    ## Parameter tuning of 'svm':
    ## 
    ## - sampling method: 10-fold cross validation 
    ## 
    ## - best parameters:
    ##  gamma cost
    ##   0.01   10
    ## 
    ## - best performance: 0.236

``` r
svm.model <- svm(Creditability~.,
                 data = train,
                 kernel='radial',
                 gamma=0.01,
                 cost=10)
summary(svm.model)
```

    ## 
    ## Call:
    ## svm(formula = Creditability ~ ., data = train, kernel = "radial", 
    ##     gamma = 0.01, cost = 10)
    ## 
    ## 
    ## Parameters:
    ##    SVM-Type:  C-classification 
    ##  SVM-Kernel:  radial 
    ##        cost:  10 
    ## 
    ## Number of Support Vectors:  383
    ## 
    ##  ( 179 204 )
    ## 
    ## 
    ## Number of Classes:  2 
    ## 
    ## Levels: 
    ##  0 1

``` r
pred.svm <- predict(svm.model, test, type='class')

# 오분류표
library(caret)
```

    ## Loading required package: ggplot2

    ## Loading required package: lattice

``` r
confusionMatrix(data = pred.svm, reference = test[,1], positive = '1')
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   0   1
    ##          0  43  33
    ##          1  41 183
    ##                                           
    ##                Accuracy : 0.7533          
    ##                  95% CI : (0.7005, 0.8011)
    ##     No Information Rate : 0.72            
    ##     P-Value [Acc > NIR] : 0.1100          
    ##                                           
    ##                   Kappa : 0.3699          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.4158          
    ##                                           
    ##             Sensitivity : 0.8472          
    ##             Specificity : 0.5119          
    ##          Pos Pred Value : 0.8170          
    ##          Neg Pred Value : 0.5658          
    ##              Prevalence : 0.7200          
    ##          Detection Rate : 0.6100          
    ##    Detection Prevalence : 0.7467          
    ##       Balanced Accuracy : 0.6796          
    ##                                           
    ##        'Positive' Class : 1               
    ## 

``` r
# ROC 그래프와 AUC
library(ROCR)
pred.svm.roc <- prediction(as.numeric(pred.svm), as.numeric(test[,1]))
plot(performance(pred.svm.roc,'tpr','fpr'))
```

![](2장.-분류-분석-2-_files/figure-markdown_github/unnamed-chunk-1-1.png)

``` r
performance(pred.svm.roc,'auc')@y.values
```

    ## [[1]]
    ## [1] 0.6795635

#### 예제 - iris

### 5절. 나이브 베이즈 분류(Naive Bayes Classification) (e1071 패키지)

#### 1. Bayes theorem

: 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리

$$
P(A\|B) = \frac{P(B\cap A)}{P(B)} = \frac{P(A)P(B\|A)}{P(B)} = \frac{P(A)P(B\|A)}{P(A)P(B\|A)+P(A^c)P(B\|A^c)}
$$

#### 2. 나이브 베이즈 분류

: 속성값에 대해 다른 속성이 독립적이라 전제했을 때 해당 속성 값이 클래스
분류에 미치는 영향을 측정

#### 3. R을 이용한 나이브 베이즈 분류 분석

-   naiveBayes(formula, data, laplace=0,…)

    -   laplace : 라플라스 보정 여부

``` r
library(e1071)
nb.model <- naiveBayes(Creditability~.,
                       data = train,
                       laplace = 0)
nb.model
```

    ## 
    ## Naive Bayes Classifier for Discrete Predictors
    ## 
    ## Call:
    ## naiveBayes.default(x = X, y = Y, laplace = laplace)
    ## 
    ## A-priori probabilities:
    ## Y
    ##         0         1 
    ## 0.3085714 0.6914286 
    ## 
    ## Conditional probabilities:
    ##    Account.Balance
    ## Y       [,1]     [,2]
    ##   0 1.875000 1.046867
    ##   1 2.847107 1.228278
    ## 
    ##    Duration.of.Credit..month.
    ## Y       [,1]     [,2]
    ##   0 25.37037 13.71341
    ##   1 18.81612 10.84529
    ## 
    ##    Payment.Status.of.Previous.Credit
    ## Y       [,1]     [,2]
    ##   0 2.171296 1.113682
    ##   1 2.768595 1.067187
    ## 
    ##    Purpose
    ## Y       [,1]     [,2]
    ##   0 2.763889 2.891011
    ##   1 2.745868 2.750397
    ## 
    ##    Credit.Amount
    ## Y       [,1]     [,2]
    ##   0 3954.116 3597.398
    ##   1 2952.895 2377.053
    ## 
    ##    Value.Savings.Stocks
    ## Y      [,1]     [,2]
    ##   0 1.62963 1.247081
    ##   1 2.35124 1.699037
    ## 
    ##    Length.of.current.employment
    ## Y       [,1]     [,2]
    ##   0 3.175926 1.207178
    ##   1 3.454545 1.220935
    ## 
    ##    Instalment.per.cent
    ## Y       [,1]     [,2]
    ##   0 3.106481 1.079515
    ##   1 2.882231 1.158538
    ## 
    ##    Sex...Marital.Status
    ## Y       [,1]      [,2]
    ##   0 2.601852 0.7337678
    ##   1 2.721074 0.6920804
    ## 
    ##    Guarantors
    ## Y       [,1]      [,2]
    ##   0 1.143519 0.4448722
    ##   1 1.136364 0.4662524
    ## 
    ##    Duration.in.Current.address
    ## Y       [,1]     [,2]
    ##   0 2.888889 1.068216
    ##   1 2.890496 1.091276
    ## 
    ##    Most.valuable.available.asset
    ## Y       [,1]     [,2]
    ##   0 2.569444 1.022897
    ##   1 2.264463 1.035747
    ## 
    ##    Age..years.
    ## Y       [,1]     [,2]
    ##   0 33.59722 11.12465
    ##   1 37.03306 11.69271
    ## 
    ##    Concurrent.Credits
    ## Y       [,1]      [,2]
    ##   0 2.597222 0.7593092
    ##   1 2.708678 0.6778415
    ## 
    ##    Type.of.apartment
    ## Y       [,1]      [,2]
    ##   0 1.898148 0.6090758
    ##   1 1.954545 0.4836884
    ## 
    ##    No.of.Credits.at.this.Bank
    ## Y       [,1]      [,2]
    ##   0 1.402778 0.5943859
    ##   1 1.448347 0.6065698
    ## 
    ##    Occupation
    ## Y       [,1]      [,2]
    ##   0 2.902778 0.6715422
    ##   1 2.863636 0.6671841
    ## 
    ##    No.of.dependents
    ## Y       [,1]      [,2]
    ##   0 1.138889 0.3466339
    ##   1 1.163223 0.3699515
    ## 
    ##    Telephone
    ## Y       [,1]      [,2]
    ##   0 1.356481 0.4800723
    ##   1 1.400826 0.4905730
    ## 
    ##    Foreign.Worker
    ## Y       [,1]      [,2]
    ##   0 1.018519 0.1351300
    ##   1 1.053719 0.2256956

``` r
pred.nb <- predict(nb.model, test[,-1],type = 'class')

# 오분류표
library(caret)
confusionMatrix(data = pred.nb, reference = test[,1], positive = '1')
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   0   1
    ##          0  53  49
    ##          1  31 167
    ##                                           
    ##                Accuracy : 0.7333          
    ##                  95% CI : (0.6795, 0.7825)
    ##     No Information Rate : 0.72            
    ##     P-Value [Acc > NIR] : 0.32910         
    ##                                           
    ##                   Kappa : 0.3793          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.05735         
    ##                                           
    ##             Sensitivity : 0.7731          
    ##             Specificity : 0.6310          
    ##          Pos Pred Value : 0.8434          
    ##          Neg Pred Value : 0.5196          
    ##              Prevalence : 0.7200          
    ##          Detection Rate : 0.5567          
    ##    Detection Prevalence : 0.6600          
    ##       Balanced Accuracy : 0.7021          
    ##                                           
    ##        'Positive' Class : 1               
    ## 

``` r
# ROC 그래프와 AUC
pred.nb.roc <- prediction(as.numeric(pred.nb),as.numeric(test[,1]))
library(ROCR)
plot(performance(pred.nb.roc,'tpr','fpr'))
```

![](2장.-분류-분석-2-_files/figure-markdown_github/unnamed-chunk-2-1.png)

``` r
performance(pred.nb.roc,'auc')@y.values
```

    ## [[1]]
    ## [1] 0.7020503

### 6절. K-NN(K-Nearest Neighbor) (class 패키지)

#### 1. K-NN 알고리즘의 원리

#### 2. K의 선택

일반적으로 **훈련 데이터 개수의 제곱근**으로 설정

#### 3. R을 이용한 K-NN 분석

-   knn(train, test, cl, k,…)

    -   cl : 훈련 데이터의 종속변수

    -   k : 이웃의 개수

``` r
library(class)
knn.3 <- knn(train[,-1], test[,-1],train[,1],k=3)
knn.26<- knn(train[,-1], test[,-1],train[,1],
             k=round(sqrt(nrow(train))))

library(caret)
confusionMatrix(knn.3, test$Creditability)
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   0   1
    ##          0  21  52
    ##          1  63 164
    ##                                         
    ##                Accuracy : 0.6167        
    ##                  95% CI : (0.559, 0.672)
    ##     No Information Rate : 0.72          
    ##     P-Value [Acc > NIR] : 1.0000        
    ##                                         
    ##                   Kappa : 0.0096        
    ##                                         
    ##  Mcnemar's Test P-Value : 0.3511        
    ##                                         
    ##             Sensitivity : 0.2500        
    ##             Specificity : 0.7593        
    ##          Pos Pred Value : 0.2877        
    ##          Neg Pred Value : 0.7225        
    ##              Prevalence : 0.2800        
    ##          Detection Rate : 0.0700        
    ##    Detection Prevalence : 0.2433        
    ##       Balanced Accuracy : 0.5046        
    ##                                         
    ##        'Positive' Class : 0             
    ## 

``` r
confusionMatrix(knn.26, test$Creditability)
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   0   1
    ##          0   8  10
    ##          1  76 206
    ##                                           
    ##                Accuracy : 0.7133          
    ##                  95% CI : (0.6586, 0.7638)
    ##     No Information Rate : 0.72            
    ##     P-Value [Acc > NIR] : 0.6292          
    ##                                           
    ##                   Kappa : 0.0644          
    ##                                           
    ##  Mcnemar's Test P-Value : 2.398e-12       
    ##                                           
    ##             Sensitivity : 0.09524         
    ##             Specificity : 0.95370         
    ##          Pos Pred Value : 0.44444         
    ##          Neg Pred Value : 0.73050         
    ##              Prevalence : 0.28000         
    ##          Detection Rate : 0.02667         
    ##    Detection Prevalence : 0.06000         
    ##       Balanced Accuracy : 0.52447         
    ##                                           
    ##        'Positive' Class : 0               
    ## 

``` r
# 최적의 k
result <- numeric()
k=1:28
for (i in k) {
  pred <- knn(train[,-1], test[,-1],train[,1],k=i)
  t <- table(pred, test$Creditability)
  result[i] <- (t[1,1]+t[2,2])/sum(t)
}
result
```

    ##  [1] 0.5766667 0.5866667 0.6166667 0.6466667 0.6333333 0.6466667 0.6533333
    ##  [8] 0.6500000 0.6566667 0.6600000 0.6666667 0.6700000 0.6800000 0.6733333
    ## [15] 0.7033333 0.6866667 0.7066667 0.7100000 0.7200000 0.7000000 0.7066667
    ## [22] 0.7233333 0.7166667 0.7133333 0.7166667 0.7033333 0.7033333 0.7033333

``` r
which(result==max(result))
```

    ## [1] 22

### 7절. 인공신경망 모형(Artificial Neural Network) (nnet 패키지)

#### 1. 특징

입력 링크에서 여러 신호를 받아서 새로운 활성화 수준을 계산하고 출력
링크로 출력 신호를 보냄

활성화 함수를 이용해 출력을 결정하며 입력신호의 가중치 합을 계산하여
임계값 비교  
-\> -1 or 1 출력

-   뉴런의 활성화 함수

    -   계단 함수

    -   부호 함수

    -   시그모이드 함수

    -   relu 함수

    -   softmax 함수

#### 2. R을 이용한 인공신경망 분석

-   nnet(formula, data, size, maxit, decay=5e-04,…)

    -   size : hidden node의 개수

    -   maxit : 학습 반복횟수, 반복 중 가장 좋은 모델을 선정

    -   decay : 가중치 감소의 모수

-   garson(mod_in) (NeuralNetTools 패키지)\\  
    모델의 변수 중요도 파악

    -   mod_in : 생성된 인공신경망 모델

``` r
library(nnet)
set.seed(1)
nn.model <- nnet(Creditability~.,
                 data = train,
                 size=2,
                 maxit=200,
                 decay=5e-04)
```

    ## # weights:  45
    ## initial  value 441.360687 
    ## iter  10 value 432.569292
    ## iter  20 value 432.522026
    ## iter  30 value 429.142744
    ## iter  40 value 424.982312
    ## iter  50 value 422.170983
    ## iter  60 value 420.583401
    ## iter  70 value 419.927488
    ## iter  80 value 419.789060
    ## iter  90 value 418.499180
    ## iter 100 value 418.270549
    ## iter 110 value 417.982946
    ## iter 120 value 417.829888
    ## iter 130 value 416.414615
    ## iter 140 value 411.658691
    ## iter 150 value 408.224482
    ## iter 160 value 403.270461
    ## iter 170 value 402.251239
    ## iter 180 value 402.054265
    ## iter 190 value 401.327957
    ## iter 200 value 401.294930
    ## final  value 401.294930 
    ## stopped after 200 iterations

``` r
summary(nn.model)
```

    ## a 20-2-1 network with 45 weights
    ## options were - entropy fitting  decay=5e-04
    ##   b->h1  i1->h1  i2->h1  i3->h1  i4->h1  i5->h1  i6->h1  i7->h1  i8->h1  i9->h1 
    ##    1.15   -4.89    5.76   -0.85    1.58    0.14   -3.78    0.25    1.44   -0.17 
    ## i10->h1 i11->h1 i12->h1 i13->h1 i14->h1 i15->h1 i16->h1 i17->h1 i18->h1 i19->h1 
    ##   -2.14    2.49    9.85   -4.93   -7.67    1.49    5.05    0.47    7.29   -3.24 
    ## i20->h1 
    ##   -1.31 
    ##   b->h2  i1->h2  i2->h2  i3->h2  i4->h2  i5->h2  i6->h2  i7->h2  i8->h2  i9->h2 
    ##    0.53    9.74    3.99   12.25   -0.03   -0.03   -2.54    1.03   -5.22    9.24 
    ## i10->h2 i11->h2 i12->h2 i13->h2 i14->h2 i15->h2 i16->h2 i17->h2 i18->h2 i19->h2 
    ##   -2.24    0.37   11.67    0.19   -4.81    0.37   -9.03   -0.65   11.56  -17.48 
    ## i20->h2 
    ##    0.18 
    ##   b->o  h1->o  h2->o 
    ##   2.18  -2.69   1.33

``` r
# 변수중요도
#install.packages('NeuralNetTools')
library(NeuralNetTools)
garson(nn.model)
```

![](2장.-분류-분석-2-_files/figure-markdown_github/unnamed-chunk-6-1.png)

``` r
pred.nn <- predict(nn.model, test[,-1], type = 'class')

# 오분류표
library(caret)
confusionMatrix(as.factor(pred.nn), test[,1],positive = '1')
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   0   1
    ##          0  19  19
    ##          1  65 197
    ##                                           
    ##                Accuracy : 0.72            
    ##                  95% CI : (0.6655, 0.7701)
    ##     No Information Rate : 0.72            
    ##     P-Value [Acc > NIR] : 0.5294          
    ##                                           
    ##                   Kappa : 0.166           
    ##                                           
    ##  Mcnemar's Test P-Value : 9.112e-07       
    ##                                           
    ##             Sensitivity : 0.9120          
    ##             Specificity : 0.2262          
    ##          Pos Pred Value : 0.7519          
    ##          Neg Pred Value : 0.5000          
    ##              Prevalence : 0.7200          
    ##          Detection Rate : 0.6567          
    ##    Detection Prevalence : 0.8733          
    ##       Balanced Accuracy : 0.5691          
    ##                                           
    ##        'Positive' Class : 1               
    ## 

``` r
# ROC그래프와 AUC
library(ROCR)
pred.nn.roc <- prediction(as.numeric(pred.nn), as.numeric(test[,1]))
plot(performance(pred.nn.roc,'tpr','fpr'))
```

![](2장.-분류-분석-2-_files/figure-markdown_github/unnamed-chunk-7-1.png)

``` r
performance(pred.nn.roc,'auc')@y.values
```

    ## [[1]]
    ## [1] 0.5691138

-   neuralnet(formula, data, algorithm, threshold, hidden, stepmax,…) (neuralnet 패키지)\\  
    탄력적 역전파가 사용되었고 인공신경망 중 빠른 알고리즘

    -   algorithm : 사용할 알고리즘 지정
        (‘backprop’,‘rprop+’,‘rprop-’,…)

    -   threshold : 훈련중단 기준 (default = 0.01)

    -   hidden : 은닉노드의 개수

    -   stepmax : 인공 신경망 훈련 수행 최대 횟수

``` r
#install.packages('neuralnet')
library(neuralnet)
```

    ## 
    ## Attaching package: 'neuralnet'

    ## The following object is masked from 'package:ROCR':
    ## 
    ##     prediction

``` r
library(caret)

data("infert")
set.seed(1)
in.part <- createDataPartition(infert$case,
                               times = 1,
                               p=0.7)
table(infert[in.part$Resample1,'case'])
```

    ## 
    ##   0   1 
    ## 119  55

``` r
parts <- as.vector(in.part$Resample1)
train.infert <- infert[parts,]
test.infert <- infert[-parts,]

nn.model2 <- neuralnet(case~age+parity+induced+spontaneous,
                       data=train.infert,
                       hidden = c(2,2),
                       algorithm = 'rprop+',
                       threshold = 0.01,
                       stepmax = 1e+5)
plot(nn.model2)


# 오분류표
set.seed(2)
test.infert$nn.model2_pred.prob <- compute(nn.model2, covariate = test.infert[,c(2:4,6)])$net.result   # 오류가 있는듯
test.infert$nn.model2_pred <- ifelse(test.infert$nn.model2_pred.prob>0.5,1,0)
confusionMatrix(as.factor(test.infert$nn.model2_pred),as.factor(test.infert[,5]))
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  0  1
    ##          0 37  9
    ##          1  9 19
    ##                                          
    ##                Accuracy : 0.7568         
    ##                  95% CI : (0.6431, 0.849)
    ##     No Information Rate : 0.6216         
    ##     P-Value [Acc > NIR] : 0.009813       
    ##                                          
    ##                   Kappa : 0.4829         
    ##                                          
    ##  Mcnemar's Test P-Value : 1.000000       
    ##                                          
    ##             Sensitivity : 0.8043         
    ##             Specificity : 0.6786         
    ##          Pos Pred Value : 0.8043         
    ##          Neg Pred Value : 0.6786         
    ##              Prevalence : 0.6216         
    ##          Detection Rate : 0.5000         
    ##    Detection Prevalence : 0.6216         
    ##       Balanced Accuracy : 0.7415         
    ##                                          
    ##        'Positive' Class : 0              
    ## 

### 추가

#### 1. XGBoost

xgboost 패키지  
xgb.DMatrix(data, label) : xgboost 학습데이터 생성

-   data : ※ matrix 형태

-   label : ※ 수치형으로 변환

xgboost() : 트리모델 생성

-   max_depth : 의사결정나무 깊이의 한도 =\> 커질수록 과적합

-   eta : step size shrinkage로 학습 단계별 가중치를 얼마나 적용할지 =\>
    낮을수록 보수적

-   nthread : 학습에 사용할 thread 수

-   nround : iteration을 몇 번 진행할지

-   objective : 어떤 목적을 가지고 학습을
    진행할지(‘binary:logistic’:이항 / ‘multi:softmax’:다항)

-   num_class : 클래스가 몇 개가 존재하는지 알려주는 값

-   verbose : 학습마다 평가값들에 대한 메세지를 출력할지(0:출력X /
    1:출력)

``` r
#install.packages('xgboost')
library(xgboost)

data(iris)
dt <- iris
dt$label <- ifelse(iris$Species=='setosa',0,
                   ifelse(iris$Species=='versicolor',1,2))

set.seed(1)
train_idx <- sample(1:nrow(iris), 0.7*nrow(iris))

# train set and fitting
train <- dt[train_idx,]
test <- dt[-train_idx,]

train_mat <- as.matrix(train[,-c(5:6)])
train_label <- train$label
dtrain <-xgb.DMatrix(data=train_mat, label=train_label)
xgb_model <- xgboost(data = dtrain, max_depth=2,eta=1,
                     nthread=2,nround=2,
                     objective='multi:softmax',
                     num_class=3,
                     verbose=0)

# test set and predict
test_mat <- as.matrix(test[,-c(5:6)])
test_label <- test$label

pred_iris <- predict(xgb_model, test_mat)
table(pred_iris,test_label)
```

    ##          test_label
    ## pred_iris  0  1  2
    ##         0 15  0  0
    ##         1  0 17  3
    ##         2  0  0 10

``` r
# 변수 중요도 
names <- dimnames(dtrain)[[2]]
importance_mat <- xgb.importance(names,model=xgb_model)
importance_mat
```

    ##         Feature        Gain      Cover Frequency
    ## 1: Petal.Length 0.916689139 0.66148334 0.5384615
    ## 2:  Petal.Width 0.081460477 0.28440380 0.3076923
    ## 3: Sepal.Length 0.001850384 0.05411286 0.1538462

``` r
xgb.plot.importance(importance_mat[1:2,])
```

![](2장.-분류-분석-2-_files/figure-markdown_github/unnamed-chunk-10-1.png)
